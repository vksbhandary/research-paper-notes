# Research papers

This is a list of recent papers which i've read already. Repository also contain short notes on their key ideas.

|  SNo. | Title |  Notes  |  Link  |
|--|--|--|--|
| 1  | XLNet: Generalized Autoregressive Pretraining for Language Understanding  |   [Notes](notes/XLNet.md)  | [arxiv](https://arxiv.org/abs/1906.08237) |
| 2 | Phrase-Based & Neural Unsupervised Machine Translation |   [Notes](notes/phrase-based-translation.md)   | [arxiv](https://arxiv.org/abs/1804.07755) |
| 3 | Unsupervised Machine Translation Using Monolingual Corpora Only | [Notes](notes/monolingual-translation.md)  | [arxiv](https://arxiv.org/abs/1711.00043) |
| 4 | Cross-lingual Language Model Pretraining |  [Notes](notes/XLM.md)  | [arxiv](https://arxiv.org/abs/1901.07291) |
| 5 | RoBERTa: A Robustly Optimized BERT Pretraining Approach |   [Medium Blog](https://towardsdatascience.com/robustly-optimized-bert-pretraining-approaches-537dc66522dd) | [arxiv](https://arxiv.org/abs/1907.11692) |
| 6 | Unsupervised Question Answering by Cloze Translation |  [Notes](notes/cloze-translation.md) | [arxiv](https://arxiv.org/abs/1906.04980) |
| 7 | Unified Language Model Pre-training for Natural Language Understanding and Generation | [Notes](notes/UNILM.md)  | [arxiv](https://arxiv.org/abs/1905.03197) |
| 8 | Attention is all you need | [Notes](notes/Attention-Is-All-You-Need.md)  | [arxiv](https://arxiv.org/abs/1706.03762) |


Next paper
 - [ ] [Improving Language Understanding
by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
 - [ ] [SuperGLUE](https://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems)
